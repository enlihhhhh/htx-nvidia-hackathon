[project]
name = "omni-mini"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "accelerate>=1.10.0",
    "aiohttp>=3.12.15",
    "blobfile>=3.0.0",
    "cairosvg>=2.8.2",
    "colorama>=0.4.6",
    "conformer==0.3.2",
    "datasets>=4.0.0",
    "decord>=0.6.0",
    "deepspeed==0.16.9",
    "diffusers>=0.35.1",
    "easydict>=1.13",
    "fastapi>=0.116.1",
    "fire>=0.7.0",
    "flash-attn",
    "flask==3.0.3",
    "gdown>=5.2.0",
    "gradio>=4.44.1",
    "huggingface-hub>=0.35.0rc0",
    "hyperpyyaml>=1.2.2",
    "immutabledict>=4.2.1",
    "jinja2>=3.1.6",
    "jsonlines>=4.0.0",
    "librosa==0.11.0",
    "litgpt",
    "loguru>=0.7.3",
    "omegaconf==2.3.0",
    "onnxruntime==1.19.0",
    "openai-whisper>=20250625",
    "packaging>=24.2",
    "pandas>=2.3.1",
    "pillow>=10.4.0",
    "pydantic==2.10.6",
    "pydub==0.25.1",
    "rich>=13.9.4",
    "sacrebleu==1.5.1",
    "sentencepiece>=0.2.1",
    "six==1.16.0",
    "snac==1.2.0",
    "soundfile==0.13.1",
    "sox>=1.5.0",
    "streamlit==1.37.1",
    "sty>=1.0.6",
    "timm>=1.0.19",
    "tokenizers==0.19.1",
    "torch==2.4.1",
    "torchaudio==2.4.1",
    "torchdyn==1.0.6",
    "torchvision==0.19.1",
    "tqdm>=4.67.1",
    "transformers>=4.44.2",
    "ujson>=5.11.0",
    "validators>=0.35.0",
    "wget>=3.2",
]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.2/flash_attn-2.8.2+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl" }
nvidia-rag = { path = "rag", editable = true }

[tool.uv]
prerelease = "allow"
